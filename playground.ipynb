{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# lib"
      ],
      "metadata": {
        "id": "ZqidW0Dl1dfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from torch import  nn,Tensor\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "l7fpcOuP1AiO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config"
      ],
      "metadata": {
        "id": "fuZmZ0E21mIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int\n",
        "    seq_len: int\n",
        "    d_model: int\n",
        "    num_heads: int\n",
        "    num_layers: int\n",
        "    d_ff: int\n",
        "    dropout: float = 0.1\n",
        "    grad_clip_norm: float = 1.0\n",
        "    lr: float = 6e-4\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 2\n",
        "    steps_per_epoch: int = 28000\n",
        "    report_interval: int = 1000000\n",
        "    betas: tuple = (0.9, 0.95)\n",
        "    weight_decay: float = 0.01\n",
        "    use_fused: bool = True"
      ],
      "metadata": {
        "id": "kCFq1IDm0dYB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kU4H5qN_0Hr4"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, prompt: str, max_new: int, device: str, seq_len: int, top_k: int = 50, temperature: float = 1.0) -> str:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "        prompt_len = tokens.size(1)\n",
        "\n",
        "        if prompt_len > seq_len:\n",
        "            tokens = tokens[:, -seq_len:]\n",
        "            prompt_len = seq_len\n",
        "\n",
        "        seq = tokens\n",
        "        for _ in range(max_new):\n",
        "            context = seq[:, -seq_len:]\n",
        "            logits = model(context)\n",
        "            logits = logits[:, -1, :]\n",
        "            logits = logits / temperature\n",
        "            top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(top_k_logits, dim=-1)\n",
        "            next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
        "            next_tok = top_k_indices.gather(-1, next_tok_idx)\n",
        "            seq = torch.cat([seq, next_tok], dim=1)\n",
        "\n",
        "        generated_text = tokenizer.decode(seq[0].tolist())\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "hPhtkBlR1iAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask: torch.Tensor):\n",
        "        h = self.ln1(x)\n",
        "        attn_out, _ = self.attn(h, h, h, attn_mask=mask)\n",
        "        x = x + attn_out\n",
        "        h = self.ln2(x)\n",
        "        return x + self.ff(h)\n",
        "\n",
        "class GPT2Simple(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.pos_emb = nn.Embedding(cfg.seq_len, cfg.d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            GPT2Block(cfg.d_model, cfg.num_heads, cfg.d_ff, cfg.dropout)\n",
        "            for _ in range(cfg.num_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "        self.head.weight = self.tok_emb.weight\n",
        "        bool_mask = torch.triu(torch.ones(cfg.seq_len, cfg.seq_len, dtype=torch.bool), diagonal=1)\n",
        "        self.register_buffer('causal_mask', bool_mask)\n",
        "\n",
        "    def forward(self, input_ids: Tensor):\n",
        "        bsz, seqlen = input_ids.size()\n",
        "        x = self.tok_emb(input_ids) + self.pos_emb(\n",
        "            torch.arange(seqlen, device=input_ids.device).unsqueeze(0).expand(bsz, -1)\n",
        "        )\n",
        "        mask = self.causal_mask[:seqlen, :seqlen]\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, mask)\n",
        "        return self.head(self.ln_f(x))"
      ],
      "metadata": {
        "id": "0BW5fg9R1PIf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load"
      ],
      "metadata": {
        "id": "LqUyU_yc25gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfg = Config(10000, 128, 768, 8, 2, 3072, batch_size=64)  # همون کانفیگ قبلی\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file='/content/drive/MyDrive/bpe-tokenizer_tinystories.json',\n",
        "    pad_token='<|pad|>'\n",
        ")\n",
        "\n",
        "\n",
        "model = GPT2Simple(cfg).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ckpt_epoch2xx.pt', map_location=device))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwuu9kqM0PVY",
        "outputId": "6ed49f52-bcd6-449c-e0b8-13c87de23f25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Simple(\n",
              "  (tok_emb): Embedding(10000, 768)\n",
              "  (pos_emb): Embedding(128, 768)\n",
              "  (blocks): ModuleList(\n",
              "    (0-1): 2 x GPT2Block(\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=768, out_features=10000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt"
      ],
      "metadata": {
        "id": "hfX4srYF1wDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"One day, Lily saw a butterfly and decided to\"\n",
        "generated = generate_text(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    prompt=prompt,\n",
        "    max_new=64,\n",
        "    device=device,\n",
        "    seq_len=cfg.seq_len,\n",
        "    temperature=0.8\n",
        ")\n",
        "\n",
        "print(\"Generated text:\\n\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNlpCWEu0aGw",
        "outputId": "f5722202-605e-4ef8-a4f6-5d50cd0fdedf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            " <|endoftext|>One day, Lily saw a butterfly and decided to follow it. She ran and ran until she saw the butterfly. The butterfly was very big and had wings. Lily wanted to catch it and run after it. She ran and ran until she tripped and fell on the grass. She hurt her knee and cried.\n",
            "\n",
            "Her mom came out and saw what happened. She\n"
          ]
        }
      ]
    }
  ]
}